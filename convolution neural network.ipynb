{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f082ff1",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408dd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a636157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55500 55500\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "lbl = []\n",
    "\n",
    "for folder in os.listdir('Gesture Image Data'):\n",
    "    for idx, image in enumerate(os.listdir(os.path.join('Gesture Image Data', folder))):\n",
    "        images.append(cv2.imread(os.path.join('Gesture Image Data', folder, image)))\n",
    "        lbl.append(folder)\n",
    "\n",
    "print(len(images),len(lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709d0676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape #pytorch accepts (channel, widht, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9462b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "for idx, img in enumerate(images):\n",
    "    images[idx] = numpy.transpose(images[idx])\n",
    "\n",
    "print(images[100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf198f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamza\\AppData\\Local\\Temp\\ipykernel_1376\\1848181980.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  images = torch.FloatTensor(images)\n"
     ]
    }
   ],
   "source": [
    "images = torch.FloatTensor(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8c92bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 62.,  65.,  63.,  ...,  60.,  58.,  56.],\n",
      "         [ 58.,  62.,  63.,  ...,  62.,  62.,  60.],\n",
      "         [ 66.,  67.,  65.,  ...,  60.,  58.,  57.],\n",
      "         ...,\n",
      "         [ 93.,  93.,  89.,  ...,  71.,  71.,  67.],\n",
      "         [ 94.,  96.,  97.,  ...,  78.,  71.,  71.],\n",
      "         [ 95.,  97., 101.,  ...,  80.,  74.,  72.]],\n",
      "\n",
      "        [[ 91.,  92.,  89.,  ...,  80.,  79.,  77.],\n",
      "         [ 85.,  90.,  90.,  ...,  84.,  84.,  83.],\n",
      "         [ 92.,  90.,  89.,  ...,  82.,  84.,  83.],\n",
      "         ...,\n",
      "         [112., 112., 108.,  ...,  90.,  90.,  88.],\n",
      "         [114., 113., 111.,  ...,  90.,  89.,  89.],\n",
      "         [115., 115., 112.,  ...,  90.,  90.,  91.]],\n",
      "\n",
      "        [[128., 129., 125.,  ..., 105., 106., 104.],\n",
      "         [122., 125., 124.,  ..., 109., 109., 109.],\n",
      "         [129., 128., 125.,  ..., 107., 108., 107.],\n",
      "         ...,\n",
      "         [133., 133., 129.,  ...,  98.,  98.,  96.],\n",
      "         [132., 132., 130.,  ..., 100.,  96.,  96.],\n",
      "         [132., 132., 132.,  ..., 100.,  97.,  96.]]])\n"
     ]
    }
   ],
   "source": [
    "print(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb978e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3672, 0.3795, 0.3798,  ..., 0.4138, 0.4018, 0.3972],\n",
      "         [0.3634, 0.3734, 0.3803,  ..., 0.4108, 0.4108, 0.4012],\n",
      "         [0.3845, 0.3936, 0.3900,  ..., 0.4066, 0.3903, 0.3880],\n",
      "         ...,\n",
      "         [0.4716, 0.4716, 0.4676,  ..., 0.4708, 0.4708, 0.4575],\n",
      "         [0.4744, 0.4836, 0.4935,  ..., 0.5016, 0.4768, 0.4768],\n",
      "         [0.4769, 0.4846, 0.5039,  ..., 0.5111, 0.4881, 0.4781]],\n",
      "\n",
      "        [[0.5390, 0.5372, 0.5365,  ..., 0.5517, 0.5472, 0.5461],\n",
      "         [0.5326, 0.5420, 0.5433,  ..., 0.5565, 0.5565, 0.5549],\n",
      "         [0.5360, 0.5287, 0.5341,  ..., 0.5557, 0.5652, 0.5649],\n",
      "         ...,\n",
      "         [0.5680, 0.5680, 0.5674,  ..., 0.5968, 0.5968, 0.6009],\n",
      "         [0.5754, 0.5692, 0.5648,  ..., 0.5787, 0.5976, 0.5976],\n",
      "         [0.5774, 0.5746, 0.5588,  ..., 0.5750, 0.5936, 0.6042]],\n",
      "\n",
      "        [[0.7581, 0.7532, 0.7536,  ..., 0.7241, 0.7343, 0.7376],\n",
      "         [0.7644, 0.7528, 0.7485,  ..., 0.7222, 0.7222, 0.7288],\n",
      "         [0.7516, 0.7520, 0.7501,  ..., 0.7251, 0.7267, 0.7283],\n",
      "         ...,\n",
      "         [0.6745, 0.6745, 0.6778,  ..., 0.6498, 0.6498, 0.6555],\n",
      "         [0.6662, 0.6649, 0.6614,  ..., 0.6430, 0.6446, 0.6446],\n",
      "         [0.6627, 0.6595, 0.6586,  ..., 0.6389, 0.6398, 0.6374]]])\n"
     ]
    }
   ],
   "source": [
    "images = F.normalize(images)\n",
    "print(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8d3b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'\n",
      " 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z'\n",
      " '_']\n"
     ]
    }
   ],
   "source": [
    "print(numpy.unique(lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41841c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36]\n"
     ]
    }
   ],
   "source": [
    "lbl = numpy.unique(lbl, return_inverse=True)[1]\n",
    "print(numpy.unique(lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8028ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = torch.LongTensor(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd48e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you have gpu\n",
    "images = images.cuda()\n",
    "lbl = lbl.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0a827",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d11dcd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ea64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(conv,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6 , 5)\n",
    "        #6@46x46\n",
    "        #after max_pool 6@23x23\n",
    "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
    "        #12@19x19\n",
    "        #after max_pool 12@9x9\n",
    "        \n",
    "        self.fc1 = nn.Linear(9*9*12 , 30)\n",
    "        self.fc2 = nn.Linear(30 , 37)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        x = x.view(-1, 9*9*12)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71c41e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = conv()\n",
    "convnet = convnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eef1d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = convnet(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20237eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0535, -0.0544,  0.0802, -0.1938, -0.1929,  0.0237, -0.1306,  0.1293,\n",
      "         -0.1188,  0.1308,  0.1929, -0.0952,  0.0249, -0.0203,  0.0157, -0.1205,\n",
      "         -0.0909,  0.1684,  0.1306,  0.1977, -0.1584, -0.1681,  0.0421, -0.0161,\n",
      "          0.1685,  0.0716,  0.0510, -0.0058,  0.0353,  0.1118,  0.1293,  0.0629,\n",
      "         -0.0147, -0.1361, -0.0363, -0.1714, -0.1788]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) 37\n"
     ]
    }
   ],
   "source": [
    "print(res , len(res[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098461f",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ff910b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(convnet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9027b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16de6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eec7084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 12/11100 [00:03<35:12,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1] loss: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 11100/11100 [00:29<00:00, 380.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,11100] loss: 1.462\n",
      "done training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for count in tqdm(range(11100)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = convnet(images[count*5 : (count+1)*5])\n",
    "        loss = criterion(output, lbl[count*5 : (count+1)*5])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "        if count%11099 ==0:\n",
    "            print('[%d,%5d] loss: %.3f' %\n",
    "                 (epoch +1, count+1, running_loss/11099))\n",
    "            \n",
    "            running_loss=0.0\n",
    "            \n",
    "print('done training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e737630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(convnet, 'cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07b3198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1260, -0.7210, -0.8749, -1.1751, -1.1001, -1.0326, -1.1290, -1.2061,\n",
      "         -0.9378, -0.9761, -0.9734, -1.0249, -1.0257, -0.9283, -0.8040, -0.7299,\n",
      "         -0.6089, -0.5065, -0.4132, -0.3496, -0.2621, -0.1874, -0.1067, -0.0161,\n",
      "          0.0455,  0.1743,  0.2912,  0.4558,  0.5812,  0.7106,  0.9250,  1.1623,\n",
      "          1.4036,  1.8113,  2.2725,  3.0600,  5.4311]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "results = convnet(images[0])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcf1744f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=972, out_features=30, bias=True)\n",
       "  (fc2): Linear(in_features=30, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('cnn.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8f95bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res  = model(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8652fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1260, -0.7210, -0.8749, -1.1751, -1.1001, -1.0326, -1.1290, -1.2061,\n",
      "         -0.9378, -0.9761, -0.9734, -1.0249, -1.0257, -0.9283, -0.8040, -0.7299,\n",
      "         -0.6089, -0.5065, -0.4132, -0.3496, -0.2621, -0.1874, -0.1067, -0.0161,\n",
      "          0.0455,  0.1743,  0.2912,  0.4558,  0.5812,  0.7106,  0.9250,  1.1623,\n",
      "          1.4036,  1.8113,  2.2725,  3.0600,  5.4311]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc9ba48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.cpu()\n",
    "res = res.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68eb5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = numpy.argmax(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c8e2cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "163a6681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1259978 , -0.7210011 , -0.8748562 , -1.1750541 , -1.1001052 ,\n",
       "       -1.032618  , -1.1290306 , -1.2061433 , -0.9377701 , -0.97611266,\n",
       "       -0.97338116, -1.0249169 , -1.0256859 , -0.9283197 , -0.80404633,\n",
       "       -0.72994834, -0.6088709 , -0.5065379 , -0.41321778, -0.3495546 ,\n",
       "       -0.26209524, -0.18744344, -0.10667558, -0.01609229,  0.04554788,\n",
       "        0.17425177,  0.29118493,  0.45575923,  0.5811957 ,  0.71061337,\n",
       "        0.92497635,  1.1622584 ,  1.4035677 ,  1.8112895 ,  2.272478  ,\n",
       "        3.0600193 ,  5.4310627 ], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc28bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class = ['0']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
